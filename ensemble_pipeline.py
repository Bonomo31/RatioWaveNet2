#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Train a TCN-based ensemble on pre-computed multi-stream datasets.

The pipeline expects, for each dataset/seed pair, a folder containing the
pre-aligned sources produced by the base classifiers. The expected layout is::

    <data_root>/<dataset>/<seed>/ensemble/
        train.npz
        test.npz
        [val.npz]

Each ``.npz`` file must expose the following arrays:

``rwn`` or ``ratiowavenet``
    Logits (or posterior probabilities) generated by RatioWaveNet. Accepted
    shapes are ``(N, C)`` or ``(N, L, C)`` where ``L`` is a temporal axis.
``tcf`` or ``tcformer``
    Logits coming from the TCFormer model with the same shape constraints as
    ``rwn``.
``signals`` or ``eeg``
    Optional raw EEG windows with shape ``(N, channels, time)``.
``labels`` or ``y``
    Integer labels for each sample.

The script will harmonise the temporal dimension across the three sources,
train a small Temporal Convolutional Network (TCN) defined in
:mod:`ensemble.tcn_ensemble` and store checkpoints/metrics under
``results/<dataset>/<seed>/ensemble``.
"""

from __future__ import annotations

import argparse
import json
import math
import os
from dataclasses import asdict
from pathlib import Path
from typing import List, Optional, Sequence, Tuple

import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, cohen_kappa_score
from torch.utils.data import DataLoader, Dataset

from ensemble import TCNEnsemble, TCNEnsembleConfig
from utils.seed import seed_everything


# ---------------------------------------------------------------------------
# Dataset utilities
# ---------------------------------------------------------------------------


def _parse_list_argument(value: Optional[str]) -> List[str]:
    if value is None:
        return []
    if isinstance(value, str):
        items = [chunk.strip() for chunk in value.split(",") if chunk.strip()]
        return items
    raise TypeError(f"Unsupported type for list argument: {type(value)!r}")


def _select_array(data: np.lib.npyio.NpzFile, keys: Sequence[str], *, required: bool = True):
    for key in keys:
        if key in data:
            return data[key]
    if required:
        raise KeyError(
            "Missing array. Expected one of "
            f"{', '.join(keys)} in the provided dataset file."
        )
    return None


def _ensure_batch_first(array: np.ndarray) -> np.ndarray:
    if array.ndim == 1:
        return array[None, :, None]
    if array.ndim == 2:
        return array[:, None, :]
    if array.ndim == 3:
        return array
    raise ValueError(f"Unsupported array shape {array.shape}. Expected 1D, 2D or 3D tensor.")


def _linear_resample(batch: np.ndarray, target_length: int) -> np.ndarray:
    if target_length <= 0:
        raise ValueError("target_length must be > 0")
    if batch.shape[1] == target_length:
        return batch
    if batch.shape[1] == 1:
        return np.repeat(batch, target_length, axis=1)

    n_samples, _, n_features = batch.shape
    old_positions = np.linspace(0.0, 1.0, batch.shape[1], dtype=np.float32)
    new_positions = np.linspace(0.0, 1.0, target_length, dtype=np.float32)
    out = np.empty((n_samples, target_length, n_features), dtype=batch.dtype)

    for i in range(n_samples):
        for j in range(n_features):
            out[i, :, j] = np.interp(new_positions, old_positions, batch[i, :, j])
    return out


def _load_split(
    path: Path,
    *,
    effective_length: Optional[int],
    target_length: Optional[int],
    use_signal: bool,
) -> Tuple["EnsembleTensorDataset", int]:
    if not path.exists():
        raise FileNotFoundError(f"Dataset split not found: {path}")

    with np.load(path) as data:
        rw_raw = _select_array(data, ["rwn", "ratiowavenet", "rwn_logits"])
        tc_raw = _select_array(data, ["tcf", "tcformer", "tcf_logits"])
        labels = _select_array(data, ["labels", "y", "targets"]).astype(np.int64)
        sig_raw = _select_array(data, ["signals", "signal", "eeg", "x"], required=use_signal)

    rw_bt = _ensure_batch_first(np.asarray(rw_raw, dtype=np.float32))
    tc_bt = _ensure_batch_first(np.asarray(tc_raw, dtype=np.float32))

    n_samples = rw_bt.shape[0]
    if tc_bt.shape[0] != n_samples:
        raise ValueError(
            "Mismatched number of samples between RatioWaveNet and TCFormer streams: "
            f"{n_samples} vs {tc_bt.shape[0]}"
        )

    lengths = {rw_bt.shape[1], tc_bt.shape[1]}
    sig_bt: Optional[np.ndarray] = None
    if use_signal:
        if sig_raw is None:
            raise KeyError(
                "Signal stream requested via --use-signal but the dataset does not "
                "expose a 'signals' array."
            )
        sig_np = np.asarray(sig_raw, dtype=np.float32)
        if sig_np.ndim != 3:
            raise ValueError(
                "Signals array must have shape (samples, channels, time). "
                f"Received {sig_np.shape}."
            )
        sig_bt = np.transpose(sig_np, (0, 2, 1))
        lengths.add(sig_bt.shape[1])

    if effective_length is not None:
        eff_len = int(effective_length)
    elif target_length is not None:
        eff_len = int(target_length)
    else:
        eff_len = int(max(lengths))

    rw_bt = _linear_resample(rw_bt, eff_len)
    tc_bt = _linear_resample(tc_bt, eff_len)
    if sig_bt is not None:
        sig_bt = _linear_resample(sig_bt, eff_len)

    rw_tensor = torch.from_numpy(np.transpose(rw_bt, (0, 2, 1)))  # (N, C, T)
    tc_tensor = torch.from_numpy(np.transpose(tc_bt, (0, 2, 1)))
    sig_tensor = None
    if sig_bt is not None:
        sig_tensor = torch.from_numpy(np.transpose(sig_bt, (0, 2, 1)))

    labels = labels.reshape(-1)
    if labels.shape[0] != n_samples:
        raise ValueError(
            "Labels must contain the same number of samples as the input streams: "
            f"{labels.shape[0]} vs {n_samples}."
        )
    label_tensor = torch.from_numpy(labels.astype(np.int64))

    dataset = EnsembleTensorDataset(rw_tensor, tc_tensor, sig_tensor, label_tensor)
    return dataset, eff_len


class EnsembleTensorDataset(Dataset):
    """Tensor-based dataset returning the three ensemble streams."""

    def __init__(
        self,
        rwn: torch.Tensor,
        tcf: torch.Tensor,
        signal: Optional[torch.Tensor],
        labels: torch.Tensor,
    ) -> None:
        if rwn.ndim != 3 or tcf.ndim != 3:
            raise ValueError("Streams must be provided as (samples, channels, time) tensors.")
        if rwn.shape[0] != tcf.shape[0]:
            raise ValueError("Streams must contain the same number of samples.")
        if signal is not None and signal.shape[0] != rwn.shape[0]:
            raise ValueError("Signal stream must contain the same number of samples as logits.")

        self.rwn = rwn.float()
        self.tcf = tcf.float()
        self.signal = None if signal is None else signal.float()
        self.labels = labels.long()

    def __len__(self) -> int:  # pragma: no cover - trivial
        return int(self.labels.shape[0])

    def __getitem__(self, idx: int):  # pragma: no cover - trivial
        sig = None if self.signal is None else self.signal[idx]
        return self.rwn[idx], self.tcf[idx], sig, self.labels[idx]

    @property
    def n_classes(self) -> int:
        return int(self.labels.max().item()) + 1

    @property
    def rwn_channels(self) -> int:
        return int(self.rwn.shape[1])

    @property
    def tcf_channels(self) -> int:
        return int(self.tcf.shape[1])

    @property
    def signal_channels(self) -> int:
        return 0 if self.signal is None else int(self.signal.shape[1])

    @property
    def length(self) -> int:
        return int(self.rwn.shape[2])


def _collate_batch(batch):
    r = torch.stack([item[0] for item in batch], dim=0)
    t = torch.stack([item[1] for item in batch], dim=0)
    signals = [item[2] for item in batch]
    if signals and signals[0] is not None:
        s = torch.stack([sig for sig in signals if sig is not None], dim=0)
    else:
        s = None
    y = torch.stack([item[3] for item in batch], dim=0)
    return r, t, s, y


# ---------------------------------------------------------------------------
# Training helpers
# ---------------------------------------------------------------------------


def _compute_class_weights(labels: torch.Tensor, n_classes: int) -> torch.Tensor:
    labels_np = labels.detach().cpu().numpy().astype(np.int64)
    counts = np.bincount(labels_np, minlength=n_classes).astype(np.float32)
    counts[counts == 0.0] = 1.0
    weights = counts.sum() / (n_classes * counts)
    return torch.from_numpy(weights)


def _to_device(tensor: Optional[torch.Tensor], device: torch.device) -> Optional[torch.Tensor]:
    if tensor is None:
        return None
    return tensor.to(device, non_blocking=True)


def _train_one_epoch(
    model: TCNEnsemble,
    loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
) -> dict:
    model.train()
    total_loss = 0.0
    total_samples = 0
    preds_list: List[torch.Tensor] = []
    labels_list: List[torch.Tensor] = []

    for rwn, tcf, sig, labels in loader:
        rwn = rwn.to(device, non_blocking=True)
        tcf = tcf.to(device, non_blocking=True)
        sig = _to_device(sig, device)
        labels = labels.to(device, non_blocking=True)

        logits = model(rwn, tcf, sig)
        loss = criterion(logits, labels)

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        batch_size = labels.size(0)
        total_samples += batch_size
        total_loss += loss.item() * batch_size
        preds_list.append(logits.argmax(dim=1).detach().cpu())
        labels_list.append(labels.detach().cpu())

    preds = torch.cat(preds_list, dim=0).numpy()
    lbls = torch.cat(labels_list, dim=0).numpy()
    accuracy = float(accuracy_score(lbls, preds))
    kappa = float(cohen_kappa_score(lbls, preds))
    avg_loss = float(total_loss / max(1, total_samples))
    return {"loss": avg_loss, "accuracy": accuracy, "kappa": kappa}


def _evaluate(
    model: TCNEnsemble,
    loader: DataLoader,
    criterion: Optional[nn.Module],
    device: torch.device,
) -> dict:
    model.eval()
    total_loss = 0.0
    total_samples = 0
    preds_list: List[torch.Tensor] = []
    labels_list: List[torch.Tensor] = []

    with torch.no_grad():
        for rwn, tcf, sig, labels in loader:
            rwn = rwn.to(device, non_blocking=True)
            tcf = tcf.to(device, non_blocking=True)
            sig = _to_device(sig, device)
            labels = labels.to(device, non_blocking=True)

            logits = model(rwn, tcf, sig)
            if criterion is not None:
                loss = criterion(logits, labels)
                batch_size = labels.size(0)
                total_loss += loss.item() * batch_size
                total_samples += batch_size

            preds_list.append(logits.argmax(dim=1).detach().cpu())
            labels_list.append(labels.detach().cpu())

    preds = torch.cat(preds_list, dim=0).numpy()
    lbls = torch.cat(labels_list, dim=0).numpy()
    accuracy = float(accuracy_score(lbls, preds))
    kappa = float(cohen_kappa_score(lbls, preds))
    avg_loss = float(total_loss / total_samples) if total_samples else None
    return {"loss": avg_loss, "accuracy": accuracy, "kappa": kappa}


# ---------------------------------------------------------------------------
# Orchestration
# ---------------------------------------------------------------------------


def _run_training_for_split(
    dataset_name: str,
    seed: int,
    args: argparse.Namespace,
    device: torch.device,
) -> dict:
    data_dir = Path(args.data_root) / dataset_name / str(seed) / "ensemble"
    output_dir = Path(args.results_root) / dataset_name / str(seed) / "ensemble"
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n=== Dataset: {dataset_name} | Seed: {seed} ===")
    print(f"Data directory   : {data_dir}")
    print(f"Output directory : {output_dir}")

    train_dataset, eff_len = _load_split(
        data_dir / args.train_file,
        effective_length=None,
        target_length=args.target_length,
        use_signal=args.use_signal,
    )
    val_dataset: Optional[EnsembleTensorDataset] = None
    if args.val_file:
        val_dataset, _ = _load_split(
            data_dir / args.val_file,
            effective_length=eff_len,
            target_length=args.target_length,
            use_signal=args.use_signal,
        )
    test_dataset, _ = _load_split(
        data_dir / args.test_file,
        effective_length=eff_len,
        target_length=args.target_length,
        use_signal=args.use_signal,
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=max(0, args.num_workers),
        pin_memory=device.type == "cuda",
        collate_fn=_collate_batch,
    )
    val_loader = (
        DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=max(0, args.num_workers),
            pin_memory=device.type == "cuda",
            collate_fn=_collate_batch,
        )
        if val_dataset is not None
        else None
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=max(0, args.num_workers),
        pin_memory=device.type == "cuda",
        collate_fn=_collate_batch,
    )

    config = TCNEnsembleConfig(
        embed_dim=args.embed_dim,
        hidden_dim=args.hidden_dim,
        levels=args.levels,
        kernel_size=args.kernel_size,
        dropout=args.dropout,
    )
    model = TCNEnsemble(
        train_dataset.rwn_channels,
        train_dataset.tcf_channels,
        signal_channels=train_dataset.signal_channels if args.use_signal else None,
        n_classes=train_dataset.n_classes,
        config=config,
    ).to(device)

    class_weights = _compute_class_weights(train_dataset.labels, train_dataset.n_classes)
    class_weights = class_weights.to(device)

    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    best_state = None
    best_metric = -math.inf
    best_epoch = -1
    best_record: Optional[dict] = None
    history: List[dict] = []

    for epoch in range(1, args.epochs + 1):
        train_metrics = _train_one_epoch(model, train_loader, criterion, optimizer, device)
        if val_loader is not None:
            val_metrics = _evaluate(model, val_loader, criterion, device)
            monitor = val_metrics["accuracy"]
        else:
            val_metrics = None
            monitor = train_metrics["accuracy"]

        history.append(
            {
                "epoch": epoch,
                "train": train_metrics,
                "val": val_metrics,
            }
        )

        print(
            f"Epoch {epoch:03d} | Train acc={train_metrics['accuracy']:.4f} "
            f"kappa={train_metrics['kappa']:.4f}"
            + (
                " | Val acc={:.4f} kappa={:.4f}".format(
                    val_metrics["accuracy"], val_metrics["kappa"]
                )
                if val_metrics is not None
                else ""
            )
        )

        if monitor > best_metric:
            best_metric = monitor
            best_epoch = epoch
            best_state = {"model": model.state_dict()}
            best_record = {
                "epoch": epoch,
                "train": train_metrics,
                "val": val_metrics,
            }

    if best_state is not None:
        model.load_state_dict(best_state["model"])

    test_metrics = _evaluate(model, test_loader, criterion, device)
    print(
        f"Test accuracy={test_metrics['accuracy']:.4f} | "
        f"kappa={test_metrics['kappa']:.4f}"
    )

    checkpoint_path = output_dir / "tcn_ensemble.pt"
    torch.save(
        {
            "model_state": model.state_dict(),
            "config": asdict(config),
            "effective_length": eff_len,
            "dataset": dataset_name,
            "seed": seed,
        },
        checkpoint_path,
    )
    print(f"Checkpoint saved to {checkpoint_path}")

    metrics_path = output_dir / "metrics.json"
    summary = {
        "dataset": dataset_name,
        "seed": seed,
        "effective_length": eff_len,
        "epochs": args.epochs,
        "best_epoch": best_epoch,
        "train_metrics": history[-1]["train"] if history else None,
        "val_metrics": history[-1]["val"] if history else None,
        "best_metrics": best_record,
        "final_metrics": history[-1] if history else None,
        "test_metrics": test_metrics,
        "history": history,
        "use_signal": args.use_signal,
        "target_length": args.target_length,
        "config": asdict(config),
    }
    with open(metrics_path, "w", encoding="utf-8") as handle:
        json.dump(summary, handle, indent=2)
    print(f"Metrics saved to {metrics_path}")

    summary["checkpoint"] = str(checkpoint_path)
    return summary


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--datasets",
        type=str,
        required=True,
        help="Comma separated list of datasets to process (e.g. 'bcic2a,bcic2b').",
    )
    parser.add_argument(
        "--seeds",
        type=str,
        required=True,
        help="Comma separated list of seeds to process (e.g. '0,1,2').",
    )
    parser.add_argument("--data-root", type=str, default="results")
    parser.add_argument("--results-root", type=str, default="results")
    parser.add_argument("--train-file", type=str, default="train.npz")
    parser.add_argument("--val-file", type=str, default=None)
    parser.add_argument("--test-file", type=str, default="test.npz")
    parser.add_argument("--target-length", type=int, default=None)
    parser.add_argument("--use-signal", action="store_true")
    parser.add_argument("--epochs", type=int, default=30)
    parser.add_argument("--batch-size", type=int, default=128)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight-decay", type=float, default=0.0)
    parser.add_argument("--embed-dim", type=int, default=16)
    parser.add_argument("--hidden-dim", type=int, default=96)
    parser.add_argument("--levels", type=int, default=5)
    parser.add_argument("--kernel-size", type=int, default=5)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--gpu-id", type=int, default=0)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--num-workers", type=int, default=max(1, (os.cpu_count() or 4) // 2))
    return parser


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()

    datasets = _parse_list_argument(args.datasets)
    seeds = [int(s) for s in _parse_list_argument(args.seeds)]
    if not datasets:
        raise ValueError("--datasets must specify at least one dataset.")
    if not seeds:
        raise ValueError("--seeds must specify at least one seed.")

    device = torch.device(
        f"cuda:{args.gpu_id}" if args.gpu_id >= 0 and torch.cuda.is_available() else "cpu"
    )
    seed_everything(args.seed)

    summaries: List[dict] = []
    for dataset_name in datasets:
        for seed in seeds:
            summary = _run_training_for_split(dataset_name, seed, args, device)
            summaries.append(summary)

    if summaries:
        print("\n=== Ensemble evaluation summary ===")
        for item in summaries:
            test = item["test_metrics"]
            print(
                f"{item['dataset']} | seed={item['seed']} | "
                f"acc={test['accuracy']:.4f} | kappa={test['kappa']:.4f}"
            )

        summary_path = Path(args.results_root) / "ensemble_summary.json"
        with open(summary_path, "w", encoding="utf-8") as handle:
            json.dump(summaries, handle, indent=2)
        print(f"Global summary saved to {summary_path}")


if __name__ == "__main__":
    main()
