{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082cb18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PROJECT_ROOT: /home/giuseppe_bonomo/RatioWaveNet2\n",
      "üóÇÔ∏è  Configs YAML: ['eegnet.yaml', 'ctnet.yaml', 'atcnet.yaml', 'ratiowavenet.yaml', 'tsseffnet.yaml', 'tcformer.yaml', 'basenet.yaml', 'mscformer.yaml', 'eegconformer.yaml', 'shallownet.yaml', 'eegtcnet.yaml']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os, sys, yaml, time, glob, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "# --- Imposta qui la cartella che contiene 'train_pipeline.py' ---\n",
    "PROJECT_ROOT = Path.cwd()  # es. Path(\"/home/giuseppe_bonomo/RatioWaveNet2\")\n",
    "if not (PROJECT_ROOT / \"train_pipeline.py\").exists():\n",
    "    print(\"‚ö†Ô∏è 'train_pipeline.py' non trovato nella dir corrente. Provo in quella superiore...\")\n",
    "    if (PROJECT_ROOT.parent / \"train_pipeline.py\").exists():\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Imposta PROJECT_ROOT alla cartella del repo (con train_pipeline.py)\")\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(\"‚úÖ PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"üóÇÔ∏è  Configs YAML:\", [p.name for p in (PROJECT_ROOT / \"configs\").glob(\"*.yaml\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c7a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================== PARAMETRI ESPERIMENTO =====================\n",
    "MODEL_NAME   = \"ratiowavenet\"        # es. \"tcformer\", \"atcnet\", \"eegnet\", \"ratiowavenet\", ...\n",
    "DATASET      = \"bcic2a\"          # \"bcic2a\", \"bcic2b\", \"hgd\", \"reh_mi\", \"bcic3\"\n",
    "USE_LOSO     = False             # True=LOSO, False=intra-soggetto\n",
    "INTERAUG     = None              # True / False / None (usa default da YAML)\n",
    "GPU_ID       = 0                 # -1 per CPU; 0/1/... per GPU\n",
    "SEED_LIST    = [0,1,2,3,4]       # Elenco seed da lanciare in sequenza\n",
    "SEED_SLEEP_S = 2                 # Pausa tra seed (secondi)\n",
    "\n",
    "SUBJECT_MODE = \"one\"             # \"all\" | \"one\" | \"list\"\n",
    "ONE_SUBJECT  = 6\n",
    "SUBJECT_LIST = [1,2,3]\n",
    "\n",
    "VERBOSE_TRAIN        = True\n",
    "PLOT_CM_PER_SUBJECT  = True\n",
    "PLOT_CM_AVERAGE      = True\n",
    "SAVE_CHECKPOINTS     = False\n",
    "\n",
    "OVERRIDE_MAX_EPOCHS  = None\n",
    "# =================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdbacab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n",
      "üîß Trainer patch applicata (toggle progress bar).\n",
      "‚úÖ Config base pronta\n",
      "dataset_name: bcic2a\n",
      "subject_ids: 6\n",
      "max_epochs: 1000\n",
      "max_epochs_2b: 500\n",
      "max_epochs_loso: 125\n",
      "max_epochs_loso_hgd: 77\n",
      "seed: 0\n",
      "z_scale: true\n",
      "preprocessing:\n",
      "  sfreq: 250\n",
      "  low_cut: null\n",
      "  high_cut: null\n",
      "  start: 0.0\n",
      "  stop: 0.0\n",
      "  batch_size: 32\n",
      "  z_scale: true\n",
      "  interaug: true\n",
      "model: RatioWaveNet\n",
      "model_kwargs:\n",
      "  F1: 32\n",
      "  temp_kernel_lengths:\n",
      "  - 20\n",
      "  - 32\n",
      "  - 64\n",
      "  d_group: 16\n",
      "  D: 2\n",
      "  pool_length_1: 8\n",
      "  pool_length_2: 7\n",
      "  dropout_conv: 0.4\n",
      "  use_group_attn: true\n",
      "  q_heads: 4\n",
      "  kv_heads: 2\n",
      "  trans_depth: 5\n",
      "  trans_dropout: 0.4\n",
      "  tcn_depth: 2\n",
      "  kernel_length_tcn: 4\n",
      "  dropout_tcn: 0.3\n",
      "  lr: 0.0009\n",
      "  beta_1: 0.5\n",
      "  weight_decay: 0.001\n",
      "  optimizer: adam\n",
      "  scheduler: true\n",
      "  warmup_epochs: 20\n",
      "  warmup_epochs_loso: 3\n",
      "  use_rdwt: true\n",
      "  rdwt_levels: 4\n",
      "  rdwt_level_choices:\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  rdwt_base_kernel_len: 16\n",
      "  rdwt_init_dilations:\n",
      "  - 1.5\n",
      "  - 1.6667\n",
      "  - 1.75\n",
      "  - 1.8\n",
      "  - 1.8333\n",
      "  - 1.8571\n",
      "  - 1.875\n",
      "  - 1.8889\n",
      "  - 1.9\n",
      "  - 1.9091\n",
      "  rdwt_soft_threshold: true\n",
      "  rdwt_threshold_init: 0.0\n",
      "  rdwt_max_scale: 4.0\n",
      "  rdwt_l2_on_logscale: 0.0\n",
      "  rdwt_use_spread_loss: false\n",
      "  rdwt_spread_lambda: 3e-3\n",
      "  rdwt_spread_gamma: 5.0\n",
      "  rdwt_temp_init: 0.8\n",
      "gpu_id: 0\n",
      "plot_cm_per_subject: true\n",
      "plot_cm_average: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Costruisce una CONFIG base (senza seed), replicando la logica in run()\n",
    "from train_pipeline import train_and_test\n",
    "import pytorch_lightning as pl\n",
    "from functools import wraps\n",
    "from pytorch_lightning.trainer.trainer import Trainer as _PLTrainer\n",
    "\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "config_path = CONFIG_DIR / f\"{MODEL_NAME}.yaml\"\n",
    "assert config_path.exists(), f\"Config non trovato: {config_path}\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    base_cfg = yaml.safe_load(f)\n",
    "\n",
    "cfg = dict(base_cfg)\n",
    "if USE_LOSO:\n",
    "    cfg[\"dataset_name\"] = f\"{DATASET}_loso\"\n",
    "    cfg[\"max_epochs\"]   = cfg[\"max_epochs_loso_hgd\"] if DATASET == \"hgd\" else cfg[\"max_epochs_loso\"]\n",
    "    if \"model_kwargs\" in cfg and \"warmup_epochs_loso\" in cfg[\"model_kwargs\"]:\n",
    "        cfg[\"model_kwargs\"][\"warmup_epochs\"] = cfg[\"model_kwargs\"][\"warmup_epochs_loso\"]\n",
    "else:\n",
    "    cfg[\"dataset_name\"] = DATASET\n",
    "    cfg[\"max_epochs\"]   = cfg[\"max_epochs_2b\"] if DATASET == \"bcic2b\" else cfg[\"max_epochs\"]\n",
    "\n",
    "cfg[\"preprocessing\"] = cfg[\"preprocessing\"][DATASET]\n",
    "cfg[\"preprocessing\"][\"z_scale\"] = cfg.get(\"z_scale\", cfg[\"preprocessing\"].get(\"z_scale\", False))\n",
    "\n",
    "if INTERAUG is True:\n",
    "    cfg[\"preprocessing\"][\"interaug\"] = True\n",
    "elif INTERAUG is False:\n",
    "    cfg[\"preprocessing\"][\"interaug\"] = False\n",
    "else:\n",
    "    cfg[\"preprocessing\"][\"interaug\"] = cfg.get(\"interaug\", cfg[\"preprocessing\"].get(\"interaug\", False))\n",
    "cfg.pop(\"interaug\", None)\n",
    "\n",
    "if SUBJECT_MODE == \"all\":\n",
    "    cfg[\"subject_ids\"] = \"all\"\n",
    "elif SUBJECT_MODE == \"one\":\n",
    "    cfg[\"subject_ids\"] = ONE_SUBJECT\n",
    "elif SUBJECT_MODE == \"list\":\n",
    "    cfg[\"subject_ids\"] = SUBJECT_LIST\n",
    "else:\n",
    "    raise ValueError(\"SUBJECT_MODE deve essere 'all' | 'one' | 'list'\")\n",
    "\n",
    "cfg[\"gpu_id\"] = GPU_ID\n",
    "cfg[\"plot_cm_per_subject\"] = bool(PLOT_CM_PER_SUBJECT)\n",
    "cfg[\"plot_cm_average\"]     = bool(PLOT_CM_AVERAGE)\n",
    "if SAVE_CHECKPOINTS:\n",
    "    cfg[\"save_checkpoint\"] = True\n",
    "\n",
    "if OVERRIDE_MAX_EPOCHS is not None:\n",
    "    cfg[\"max_epochs\"] = int(OVERRIDE_MAX_EPOCHS)\n",
    "\n",
    "os.environ[\"PL_TRAIN_PROGRESS_BAR\"] = \"1\" if VERBOSE_TRAIN else \"0\"\n",
    "if not hasattr(_PLTrainer, \"_patched_progress_bar\"):\n",
    "    _orig_init = _PLTrainer.__init__\n",
    "    @wraps(_orig_init)\n",
    "    def _wrapped_init(self, *args, **kwargs):\n",
    "        kwargs.setdefault(\"enable_progress_bar\", (os.environ.get(\"PL_TRAIN_PROGRESS_BAR\",\"1\") == \"1\"))\n",
    "        kwargs.setdefault(\"enable_model_summary\", False)\n",
    "        kwargs.setdefault(\"log_every_n_steps\", 1)\n",
    "        return _orig_init(self, *args, **kwargs)\n",
    "    _PLTrainer.__init__ = _wrapped_init\n",
    "    _PLTrainer._patched_progress_bar = True\n",
    "    print(\"üîß Trainer patch applicata (toggle progress bar).\")\n",
    "\n",
    "print(\"‚úÖ Config base pronta\")\n",
    "print(yaml.dump(cfg, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0f598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Patch write_summary attivata ‚Üí verr√† creato 'results.txt' in ogni run.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Patch write_summary per creare 'results.txt' compatibile\n",
    "from utils import metrics as _metrics_mod\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not hasattr(_metrics_mod, \"_write_summary_patched\"):\n",
    "    _orig_write_summary = _metrics_mod.write_summary\n",
    "\n",
    "    def write_results_txt(result_dir, model_name, dataset_name, subject_ids, param_count,\n",
    "                          test_accs, test_losses, test_kappas, train_times, test_times, response_times):\n",
    "        result_dir = Path(result_dir)\n",
    "        lines = []\n",
    "        lines.append(f\"#Params: {param_count}\")\n",
    "        if isinstance(subject_ids, (list, tuple)):\n",
    "            for sid, acc in zip(subject_ids, test_accs):\n",
    "                try:\n",
    "                    sid_int = int(sid)\n",
    "                except Exception:\n",
    "                    sid_int = sid\n",
    "                lines.append(f\"Subject {sid_int} => Test Acc: {float(acc):.4f}\")\n",
    "        if len(test_accs):\n",
    "            lines.append(f\"Average Test Accuracy: {np.mean(test_accs):.4f} ¬± {np.std(test_accs):.4f}\")\n",
    "        if len(test_kappas):\n",
    "            lines.append(f\"Average Test Kappa: {np.mean(test_kappas):.4f} ¬± {np.std(test_kappas):.4f}\")\n",
    "        (result_dir / \"results.txt\").write_text(\"\\n\".join(lines))\n",
    "\n",
    "    def _patched_write_summary(result_dir, model_name, dataset_name, subject_ids, param_count,\n",
    "                               test_accs, test_losses, test_kappas, train_times, test_times, response_times):\n",
    "        _orig_write_summary(result_dir, model_name, dataset_name, subject_ids, param_count,\n",
    "                            test_accs, test_losses, test_kappas, train_times, test_times, response_times)\n",
    "        write_results_txt(result_dir, model_name, dataset_name, subject_ids, param_count,\n",
    "                          test_accs, test_losses, test_kappas, train_times, test_times, response_times)\n",
    "\n",
    "    _metrics_mod.write_summary = _patched_write_summary\n",
    "    _metrics_mod._write_summary_patched = True\n",
    "    print(\"üìù Patch write_summary attivata ‚Üí verr√† creato 'results.txt' in ogni run.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Patch write_summary gi√† attiva.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Avvio multi-seed: [0, 1, 2, 3, 4]\n",
      "\n",
      "==================== Seed 0 ====================\n",
      "\n",
      ">>> Training on subject: 6\n",
      "Setting all random seeds to 0, cuda_available=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giuseppe_bonomo/miniconda3/envs/tcformer310/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.48it/s, val_loss=0.722, val_acc=0.753, train_loss=0.0544, train_acc=0.997]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.48it/s, val_loss=0.722, val_acc=0.753, train_loss=0.0544, train_acc=0.997]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 16.08it/s]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       Test metric             DataLoader 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        test_acc            0.7534722089767456\n",
      "       test_kappa           0.6712962985038757\n",
      "        test_loss            0.722270667552948\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "=== Summary ===\n",
      "Average Test Accuracy: 75.35 ¬± 0.00\n",
      "Average Test Kappa:    0.671 ¬± 0.000\n",
      "Average Test Loss:     0.722 ¬± 0.000\n",
      "Total Training Time: 33.34 min\n",
      "Average Response Time: 33.99 ms\n",
      "‚úÖ Seed 0 completato. Dir: /home/giuseppe_bonomo/RatioWaveNet2/results/RatioWaveNet_bcic2a_seed-0_aug-True_GPU0_20251002_0914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Seed 1 ====================\n",
      "\n",
      ">>> Training on subject: 6\n",
      "Setting all random seeds to 1, cuda_available=True\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.42it/s, val_loss=0.747, val_acc=0.701, train_loss=0.0573, train_acc=0.998]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.42it/s, val_loss=0.747, val_acc=0.701, train_loss=0.0573, train_acc=0.998]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 16.25it/s]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       Test metric             DataLoader 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        test_acc            0.7013888955116272\n",
      "       test_kappa           0.6018518209457397\n",
      "        test_loss           0.7473905086517334\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "=== Summary ===\n",
      "Average Test Accuracy: 70.14 ¬± 0.00\n",
      "Average Test Kappa:    0.602 ¬± 0.000\n",
      "Average Test Loss:     0.747 ¬± 0.000\n",
      "Total Training Time: 33.43 min\n",
      "Average Response Time: 33.97 ms\n",
      "‚úÖ Seed 1 completato. Dir: /home/giuseppe_bonomo/RatioWaveNet2/results/RatioWaveNet_bcic2a_seed-1_aug-True_GPU0_20251002_0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Seed 2 ====================\n",
      "\n",
      ">>> Training on subject: 6\n",
      "Setting all random seeds to 2, cuda_available=True\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [00:00<00:00,  5.87it/s, val_loss=1.400, val_acc=0.240, train_loss=1.380, train_acc=0.297]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giuseppe_bonomo/miniconda3/envs/tcformer310/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found on stim channel stim\n",
      "Event IDs: [1 2 3 4]\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n",
      "Using data from preloaded Raw for 48 events and 1000 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 16.74it/s]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       Test metric             DataLoader 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        test_acc            0.2430555522441864\n",
      "       test_kappa          -0.009259223937988281\n",
      "        test_loss           1.3927446603775024\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Multi-seed loop\n",
    "from datetime import datetime\n",
    "\n",
    "results_root = PROJECT_ROOT / \"results\"\n",
    "seed_to_dir = {}\n",
    "\n",
    "print(\"üöÄ Avvio multi-seed:\", SEED_LIST)\n",
    "for sd in SEED_LIST:\n",
    "    run_cfg = dict(cfg)\n",
    "    run_cfg[\"seed\"] = int(sd)\n",
    "    start = time.time()\n",
    "    print(f\"\\n==================== Seed {sd} ====================\")\n",
    "    train_and_test(run_cfg)\n",
    "    pattern = f\"{run_cfg.get('model', MODEL_NAME)}_{run_cfg['dataset_name']}_seed-{sd}_*\"\n",
    "    candidates = sorted(results_root.glob(pattern), key=os.path.getmtime)\n",
    "    if not candidates:\n",
    "        candidates = sorted(results_root.glob(f\"*{run_cfg['dataset_name']}*seed-{sd}*\"), key=os.path.getmtime)\n",
    "    latest = candidates[-1] if candidates else None\n",
    "    seed_to_dir[sd] = latest\n",
    "    print(f\"‚úÖ Seed {sd} completato. Dir: {latest}\")\n",
    "    time.sleep(SEED_SLEEP_S)\n",
    "\n",
    "print(\"üìÇ seed_to_dir:\", seed_to_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe1b135",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['subject'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2438789/501678614.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"subject\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSEED_LIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"seed-{sd}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_parsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0macc_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf_subject_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subject\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mdf_subject_seed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_subject_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdf_subject_seed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"std\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdf_subject_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tcformer310/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_format_argument_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tcformer310/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6008\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6009\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6012\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6015\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['subject'] are in the columns\""
     ]
    }
   ],
   "source": [
    "\n",
    "# Aggregazione: soggetto √ó seed + riassunto per seed\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_results_txt(path):\n",
    "    txt = Path(path).read_text()\n",
    "    subj = {}\n",
    "    for m in re.finditer(r\"Subject\\s+(\\d+)\\s*=>\\s*Test Acc:\\s*([0-9]+\\.?[0-9]*)\", txt):\n",
    "        sid = int(m.group(1)); acc = float(m.group(2))\n",
    "        subj[sid] = acc\n",
    "    params = None\n",
    "    m = re.search(r\"#Params:\\s*([0-9]+)\", txt)\n",
    "    if m: params = int(m.group(1))\n",
    "    acc_mean = acc_std = kappa_mean = kappa_std = None\n",
    "    m = re.search(r\"Average Test Accuracy:\\s*([0-9]+\\.?[0-9]*)\\s*¬±\\s*([0-9]+\\.?[0-9]*)\", txt)\n",
    "    if m: acc_mean, acc_std = float(m.group(1)), float(m.group(2))\n",
    "    m = re.search(r\"Average Test Kappa:\\s*([0-9]+\\.?[0-9]*)\\s*¬±\\s*([0-9]+\\.?[0-9]*)\", txt)\n",
    "    if m: kappa_mean, kappa_std = float(m.group(1)), float(m.group(2))\n",
    "    return {\"params\": params, \"subj\": subj, \"acc_mean\": acc_mean, \"acc_std\": acc_std,\n",
    "            \"kappa_mean\": kappa_mean, \"kappa_std\": kappa_std}\n",
    "\n",
    "seed_parsed = {}\n",
    "for sd, d in seed_to_dir.items():\n",
    "    if d is None: continue\n",
    "    rfile = Path(d) / \"results.txt\"\n",
    "    if rfile.exists():\n",
    "        seed_parsed[sd] = parse_results_txt(rfile)\n",
    "\n",
    "all_subjects = sorted({sid for x in seed_parsed.values() for sid in x[\"subj\"].keys()})\n",
    "acc_mat = []\n",
    "for sid in all_subjects:\n",
    "    row = {\"subject\": sid}\n",
    "    for sd in SEED_LIST:\n",
    "        row[f\"seed-{sd}\"] = seed_parsed.get(sd, {}).get(\"subj\", {}).get(sid, np.nan)\n",
    "    acc_mat.append(row)\n",
    "df_subject_seed = pd.DataFrame(acc_mat).set_index(\"subject\").sort_index()\n",
    "df_subject_seed[\"mean\"] = df_subject_seed.mean(axis=1, skipna=True)\n",
    "df_subject_seed[\"std\"]  = df_subject_seed.std(axis=1, ddof=0, skipna=True)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(\"### üìä Accuracy per **soggetto √ó seed** (con media & std)\"))\n",
    "display(df_subject_seed)\n",
    "\n",
    "rows = []\n",
    "for sd in SEED_LIST:\n",
    "    info = seed_parsed.get(sd, {})\n",
    "    rows.append({\n",
    "        \"seed\": sd,\n",
    "        \"#params\": info.get(\"params\"),\n",
    "        \"acc_mean\": info.get(\"acc_mean\"),\n",
    "        \"acc_std\": info.get(\"acc_std\"),\n",
    "        \"kappa_mean\": info.get(\"kappa_mean\"),\n",
    "        \"kappa_std\": info.get(\"kappa_std\"),\n",
    "    })\n",
    "df_seed_summary = pd.DataFrame(rows).set_index(\"seed\")\n",
    "display(Markdown(\"### üßæ Riassunto **per seed**\"))\n",
    "display(df_seed_summary)\n",
    "\n",
    "export_root = PROJECT_ROOT / \"results_aggregates\"\n",
    "export_root.mkdir(parents=True, exist_ok=True)\n",
    "csv1 = export_root / f\"{MODEL_NAME}_{cfg['dataset_name']}_subjects_by_seed.csv\"\n",
    "csv2 = export_root / f\"{MODEL_NAME}_{cfg['dataset_name']}_seed_summary.csv\"\n",
    "df_subject_seed.to_csv(csv1, float_format=\"%.4f\")\n",
    "df_seed_summary.to_csv(csv2, float_format=\"%.4f\")\n",
    "print(\"üíæ CSV salvati in:\", export_root)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcformer310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
