{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082cb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, yaml, time, glob, shutil\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "# Point to the root of your repository (where train_pipeline.py lives)\n",
    "PROJECT_ROOT = Path.cwd()  # <- change this if needed, e.g. Path(\"/home/giuseppe_bonomo/RatioWaveNet2\")\n",
    "if not (PROJECT_ROOT / \"train_pipeline.py\").exists():\n",
    "    print(\"Couldn't find train_pipeline.py in current directory. Trying parent directory...\")\n",
    "    if (PROJECT_ROOT.parent / \"train_pipeline.py\").exists():\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    else:\n",
    "        print(\"Please set PROJECT_ROOT to the folder containing train_pipeline.py\")\n",
    "        raise FileNotFoundError(\"train_pipeline.py not found\")\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Light sanity print\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Available configs:\", [p.name for p in (PROJECT_ROOT / \"configs\").glob(\"*.yaml\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Choose your experiment parameters ---\n",
    "# Model name must match a YAML in configs/, e.g. \"tcformer\", \"eegnet\", etc.\n",
    "MODEL_NAME   = \"ratiowavenet\"  \n",
    "DATASET      = \"bcic2a\"        # options in your code: \"bcic2a\", \"bcic2b\", \"hgd\"\n",
    "USE_LOSO     = False           # True = subject-independent; False = intra-subject\n",
    "GPU_ID       = 0               # use -1 for CPU in your script's logic (mapped to DDP auto when -1)\n",
    "INTERAUG     = True            # True / False / None (None uses YAML default)\n",
    "SEED_LIST    = [0,1,2,3,4]     # list of seeds to run (e.g. [0,1,2,3,4]); or None to use YAML default\n",
    "SEED_SLEEP_S = 2    \n",
    "VERBOSE_TRAIN= True            # True -> show Lightning progress bar; False -> print only final summary\n",
    "PLOT_CM_PER_SUBJECT = True     # per-subject confusion matrices\n",
    "PLOT_CM_AVERAGE     = True     # average confusion matrix\n",
    "\n",
    "# --- Subject selection mode ---\n",
    "# \"all\" -> use all subjects;\n",
    "# \"one\" -> set ONE_SUBJECT below;\n",
    "# \"list\"-> set SUBJECT_LIST below.\n",
    "SUBJECT_MODE = \"one\"           # \"all\" | \"one\" | \"list\"\n",
    "ONE_SUBJECT  = 1               # used when SUBJECT_MODE == \"one\"\n",
    "SUBJECT_LIST = [1, 2, 3]       # used when SUBJECT_MODE == \"list\"\n",
    "\n",
    "# --- Optional overrides (if you want to tweak runs quickly) ---\n",
    "OVERRIDE_MAX_EPOCHS = None     # set an int to override epochs (e.g., 50); or None to keep YAML/logic\n",
    "SAVE_CHECKPOINTS    = False    # if you want to toggle saving checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This mirrors the logic inside run(), but driven by notebook variables.\n",
    "from train_pipeline import train_and_test\n",
    "from types import SimpleNamespace\n",
    "\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "config_path = CONFIG_DIR / f\"{MODEL_NAME}.yaml\"\n",
    "assert config_path.exists(), f\"Config not found: {config_path}\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# --- Apply LOSO / intra logic for dataset & epochs ---\n",
    "if USE_LOSO:\n",
    "    config[\"dataset_name\"] = f\"{DATASET}_loso\"\n",
    "    config[\"max_epochs\"]   = config[\"max_epochs_loso_hgd\"] if DATASET == \"hgd\" else config[\"max_epochs_loso\"]\n",
    "    # Warmup override for LOSO, if present\n",
    "    if \"model_kwargs\" in config and \"warmup_epochs_loso\" in config[\"model_kwargs\"]:\n",
    "        config[\"model_kwargs\"][\"warmup_epochs\"] = config[\"model_kwargs\"][\"warmup_epochs_loso\"]\n",
    "else:\n",
    "    config[\"dataset_name\"] = DATASET\n",
    "    config[\"max_epochs\"]   = config[\"max_epochs_2b\"] if DATASET == \"bcic2b\" else config[\"max_epochs\"]\n",
    "\n",
    "# --- Preprocessing per-dataset ---\n",
    "config[\"preprocessing\"] = config[\"preprocessing\"][DATASET]\n",
    "config[\"preprocessing\"][\"z_scale\"] = config.get(\"z_scale\", config[\"preprocessing\"].get(\"z_scale\", False))\n",
    "\n",
    "# --- Inter-trial augmentation override ---\n",
    "if INTERAUG is True:\n",
    "    config[\"preprocessing\"][\"interaug\"] = True\n",
    "elif INTERAUG is False:\n",
    "    config[\"preprocessing\"][\"interaug\"] = False\n",
    "else:\n",
    "    # use the top-level default \"interaug\" if present\n",
    "    config[\"preprocessing\"][\"interaug\"] = config.get(\"interaug\", config[\"preprocessing\"].get(\"interaug\", False))\n",
    "# remove top-level for cleanliness\n",
    "config.pop(\"interaug\", None)\n",
    "\n",
    "# --- Subject selection ---\n",
    "if SUBJECT_MODE == \"all\":\n",
    "    config[\"subject_ids\"] = \"all\"\n",
    "elif SUBJECT_MODE == \"one\":\n",
    "    config[\"subject_ids\"] = ONE_SUBJECT\n",
    "elif SUBJECT_MODE == \"list\":\n",
    "    config[\"subject_ids\"] = SUBJECT_LIST\n",
    "else:\n",
    "    raise ValueError(\"SUBJECT_MODE must be one of: 'all', 'one', 'list'\")\n",
    "\n",
    "# --- Seed & GPU ---\n",
    "config[\"gpu_id\"] = GPU_ID\n",
    "if SEED is not None:\n",
    "    config[\"seed\"] = SEED\n",
    "\n",
    "# --- Optional epoch override ---\n",
    "if OVERRIDE_MAX_EPOCHS is not None:\n",
    "    config[\"max_epochs\"] = int(OVERRIDE_MAX_EPOCHS)\n",
    "\n",
    "# --- Plotting toggles ---\n",
    "config[\"plot_cm_per_subject\"] = bool(PLOT_CM_PER_SUBJECT)\n",
    "config[\"plot_cm_average\"]     = bool(PLOT_CM_AVERAGE)\n",
    "\n",
    "# --- Optional checkpoint saving ---\n",
    "if SAVE_CHECKPOINTS:\n",
    "    config[\"save_checkpoint\"] = True\n",
    "\n",
    "# --- Verbosity control (Lightning progress bar / logs) ---\n",
    "# Your train_pipeline sets logger=False. We can reduce noise further by disabling the progress bar.\n",
    "# We'll use a simple flag that your code will read via environment variable.\n",
    "os.environ[\"PL_TRAIN_PROGRESS_BAR\"] = \"1\" if VERBOSE_TRAIN else \"0\"\n",
    "\n",
    "display(Markdown(\"### Config ready\"))\n",
    "print(yaml.dump(config, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We dynamically patch the Trainer kwargs to respect our VERBOSE_TRAIN flag.\n",
    "# This avoids editing your file on disk. If you prefer, you can permanently add:\n",
    "#   enable_progress_bar=bool(os.environ.get(\"PL_TRAIN_PROGRESS_BAR\",\"1\")==\"1\")\n",
    "#\n",
    "# Below is a context manager to monkey-patch pytorch_lightning.Trainer.__init__ once per session.\n",
    "import pytorch_lightning as pl\n",
    "from functools import wraps\n",
    "from pytorch_lightning.trainer.trainer import Trainer as _PLTrainer\n",
    "\n",
    "if not hasattr(_PLTrainer, \"_patched_progress_bar\"):\n",
    "    _orig_init = _PLTrainer.__init__\n",
    "\n",
    "    @wraps(_orig_init)\n",
    "    def _wrapped_init(self, *args, **kwargs):\n",
    "        # Inject our toggle unless user already set it\n",
    "        if \"enable_progress_bar\" not in kwargs:\n",
    "            kwargs[\"enable_progress_bar\"] = (os.environ.get(\"PL_TRAIN_PROGRESS_BAR\",\"1\") == \"1\")\n",
    "        if \"enable_model_summary\" not in kwargs:\n",
    "            kwargs[\"enable_model_summary\"] = False  # less noise in notebooks\n",
    "        if \"log_every_n_steps\" not in kwargs:\n",
    "            kwargs[\"log_every_n_steps\"] = 1\n",
    "        return _orig_init(self, *args, **kwargs)\n",
    "\n",
    "    _PLTrainer.__init__ = _wrapped_init\n",
    "    _PLTrainer._patched_progress_bar = True\n",
    "    print(\"Patched PyTorch Lightning Trainer to honor VERBOSE_TRAIN toggle.\")\n",
    "else:\n",
    "    print(\"Trainer already patched in this session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RUN ---\n",
    "start = time.time()\n",
    "print(\"Starting training & testing...\")\n",
    "train_and_test(config)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Done in {elapsed/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to locate the most recent results folder produced by this run and preview assets.\n",
    "from datetime import datetime\n",
    "\n",
    "results_root = PROJECT_ROOT / \"results\"\n",
    "if not results_root.exists():\n",
    "    print(\"No results folder found yet.\")\n",
    "else:\n",
    "    # Pick the latest folder matching current model+dataset\n",
    "    candidates = sorted(results_root.glob(f\"{config['model']}_{config['dataset_name']}_seed-*\"), key=os.path.getmtime)\n",
    "    if not candidates:\n",
    "        # your script names result folders with f\"{model_name}_{dataset_name}_seed-...\"\n",
    "        # but in this notebook, config['model'] may live under config['model_kwargs'] in your YAML.\n",
    "        # Fall back to a looser glob if strict one misses:\n",
    "        candidates = sorted(results_root.glob(f\"*{config['dataset_name']}*\"), key=os.path.getmtime)\n",
    "\n",
    "    if not candidates:\n",
    "        print(\"No matching result directories found.\")\n",
    "    else:\n",
    "        latest = candidates[-1]\n",
    "        print(\"Latest result dir:\", latest)\n",
    "\n",
    "        # Show confusion matrices found\n",
    "        cm_dir = latest / \"confmats\"\n",
    "        if cm_dir.exists():\n",
    "            for img_path in sorted(cm_dir.glob(\"*.png\"))[:12]:\n",
    "                display(Markdown(f\"**{img_path.name}**\"))\n",
    "                display(Image(filename=str(img_path)))\n",
    "        else:\n",
    "            print(\"No 'confmats' folder found.\")\n",
    "\n",
    "        # Show curves if any\n",
    "        curves_dir = latest / \"curves\"\n",
    "        if curves_dir.exists():\n",
    "            for img_path in sorted(curves_dir.glob(\"*.png\"))[:12]:\n",
    "                display(Markdown(f\"**{img_path.name}**\"))\n",
    "                display(Image(filename=str(img_path)))\n",
    "        else:\n",
    "            print(\"No 'curves' folder found.\")\n",
    "\n",
    "        # Try to display a summary text/CSV if your writer created one\n",
    "        # (adjust filenames if your write_summary uses a specific pattern)\n",
    "        summary_txt = list(latest.glob(\"summary*.txt\")) + list(latest.glob(\"summary*.csv\"))\n",
    "        for s in summary_txt:\n",
    "            display(Markdown(f\"### {s.name}\"))\n",
    "            try:\n",
    "                print(s.read_text())\n",
    "            except Exception as e:\n",
    "                print(\"Couldn't read summary file:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcformer310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
